---
slug: parallelization
title: Parallelization
authors: [amamirov]
tags: [research, gpu, hpc]
sidebar_position: 3
---

Hi, I am Akhmad. In this post, I share about Parallelization and memory optimization. Rest assured, my content is not AI-generated, though it has been revised for grammar and vocabulary using ChatGPT.

**Please note this is currently a draft.**

Parallelization is a crucial strategy for training large models at scale. As model sizes continue to grow, they will exceed the memory capacity of current GPUs or TPUs. Simply adding more resources will not suffice to scale the training effectively.

There are several approaches available, including data parallelism (DP), model parallelism (MP), pipeline parallelism (PP), CPU offloading, and ZeRO memory optimization and etc. . Each approach has distinct memory requirements and introduces varying levels of communication overhead.

## Data Parallelism

## Model Parallelism

## Pipeline Parallelism

## CPU offloading

## Zero Memory optimization

## SIMD, MIMD
